[
{
	"uri": "/",
	"title": "OpenSearch Log Analytics Workshop",
	"tags": [],
	"description": "",
	"content": "OpenSearch Log Analytics Workshop These are workshops and resources designed to help you use Amazon OpenSearch Service for Log Analytics.\nIn the workshop OpenSearch Log Analytics you will learn how to perform log analytics via. Amazon OpenSearch Service. You will explore the basics of ingesting, analyzing and visualizing data.\nIn the workshop CloudWatch Log Collection you will learn how to send logs - in real time - from AWS CloudWatch to Amazon OpenSearch Service. Once in OpenSearch you can analyze your CloudWatch logs and help identify errors and issues.\nIn the workshop Fluentd Log Ingestion you will learn how to use Fluentd an Opensource data collector to send logs in real time to Amazon OpenSearch Service.\nIn the workshop Anomaly Detection with MSK you will learn how to use the anomaly detection feature in OpenSearch to identify potential anomalous data. Additionally, you will learn a method of ingesting data into OpenSearch in real time via. Amazon Managed Streaming for Apache Kafka (MSK) and a Lambda function.\nIn the workshop Cross Cluster Replication you will learn how to set up replication between OpenSearch Service domains to ensure business continuity and allows you to replicate data across geographically separated locations to reduce latency.\nIn the workshop OpenSearch Alerting you will learn how to set up alerts in Amazon OpenSearch. This allows you to get a notification when a condition is met in your log data.\n"
},
{
	"uri": "/anomaly-detection-w-msk/1_getting_started.html",
	"title": "1. Getting Started",
	"tags": [],
	"description": "",
	"content": "An AWS account and access to the AWS web console is required to complete this workshop.\nIf you are attending an AWS hosted event (re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee) follow the instructions provided at AWS Event\nIf you are running this workshop on your own follow the instructions provided at Self Paced\n"
},
{
	"uri": "/open-search-fluentd/1_getting_started.html",
	"title": "1. Getting Started",
	"tags": [],
	"description": "",
	"content": "An AWS account and access to the AWS web console is required to complete this workshop.\nIf you are attending an AWS hosted event (re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee) follow the instructions provided at AWS Event\nIf you are running this workshop on your own follow the instructions provided at Self Paced\n"
},
{
	"uri": "/collect-log-cloud-watch/1_getting_started.html",
	"title": "1. Getting started",
	"tags": [],
	"description": "",
	"content": "An AWS account and access to the AWS web console is required to complete this workshop.\nIf you are are attending an AWS hosted event (re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee) follow the instructions provided at AWS Event\nIf you are running this workshop on your own follow the instructions provided at Self Paced\n"
},
{
	"uri": "/open-search-alerting/1_getting_started.html",
	"title": "1. Getting started",
	"tags": [],
	"description": "",
	"content": "An AWS account and access to the AWS web console is required to complete this workshop.\nIf you are attending an AWS hosted event (re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee) follow the instructions provided at AWS Event\nIf you are running this workshop on your own follow the instructions provided at Self Paced\n"
},
{
	"uri": "/open-search-cross-cluster-replication/1_getting_started.html",
	"title": "1. Getting started",
	"tags": [],
	"description": "",
	"content": "An AWS account and access to the AWS web console is required to complete this workshop.\nIf you are attending an AWS hosted event (re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee) follow the instructions provided at AWS Event\nIf you are running this workshop on your own follow the instructions provided at Self Paced\n"
},
{
	"uri": "/open-search-log-analytics/1_getting_started.html",
	"title": "1. Getting started",
	"tags": [],
	"description": "",
	"content": "An AWS account and access to the AWS web console is required to complete this workshop.\nIf you are attending an AWS hosted event (re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee) follow the instructions provided at AWS Event\nIf you are running this workshop on your own follow the instructions provided at Self Paced\n"
},
{
	"uri": "/open-search-log-analytics/",
	"title": "1. OpenSearch Log Analytics",
	"tags": [],
	"description": "",
	"content": "OpenSearch Log Analytics Workshop This workshop is an introduction to log analytics with Amazon OpenSearch Service. OpenSearch allows for full text search on logs indexed by the service. In this workshop we use sample Spark logs to learn about log analytics with Amazon OpenSearch Service.\nIn this workshop we will implement the following architecture\nThe architecture uses a Python application run from a Cloud9 development environment to simulate a log producer. The application will send 2,000 logs that we will later analyze. The Python application will send the logs to Kinesis Data Firehose. Kinesis Data Firehose will ingest the log into OpenSearch. Once the logs are ingested we will use OpenSearch Dashboard to analyze our logs\nWhen you are ready to being the lab navigate to Getting started\n"
},
{
	"uri": "/collect-log-cloud-watch/",
	"title": "2. CloudWatch Log Collection",
	"tags": [],
	"description": "",
	"content": "Collect AWS CloudWatch Logs for OpenSearch This workshop covers how to send logs from CloudWatch to OpenSearch. CloudWatch can be a rich source of log for various AWS services. Integrating logs from CloudWatch into OpenSearch can improves your ability to use OpenSearch for various log analytics use cases.\nIn this workshop we will implement the following architecture:\nThe architecture uses: two Glue Jobs to generate CloudWatch logs, Lambda functions to send log data from CloudWatch to OpenSearch. Once the CloudWatch log data is in OpenSearch we will search for log messages produced by the Glue Jobs.\nWhen you are ready to being the lab navigate to Getting started\n"
},
{
	"uri": "/anomaly-detection-w-msk/2_environment_set_up.html",
	"title": "2. Environment Set Up",
	"tags": [],
	"description": "",
	"content": "You will need to deploy a few services and configure your AWS environment before you can get started.\nYou will need to complete the following set up steps\n Create / deploy the required AWS services for this lab (MSK Cluster, OpenSearch domain \u0026hellip;) Configure a network security group to allow network traffic between Cloud9 and MSK Create a Kafka topic Create an OpenSearch index Map a IAM user to OpenSearch role  To begin the environment step up and configuration follow the steps in the CloudFormation (Automated)\n"
},
{
	"uri": "/collect-log-cloud-watch/2_environment_set_up.html",
	"title": "2. Environment Set Up",
	"tags": [],
	"description": "",
	"content": "We need to deploy a few services and configure our AWS environment before we can get started.\nWe will need to complete the following set up steps\n Create an OpenSearch Domain Create an IAM Role Map IAM Role to OpenSearch Role Deploy Glue Jobs to Generate CloudWatch Logs  Follow the instructions below for each step\nCreate an OpenSearch Domain  Go to the OpenSearch Console Click on Create domain  Enter the name workshop-domain for the OpenSearch Domain Under the deployment type section, select Development and testing Under the network section, select Public access Under the fine-grained access control section select Create master user Enter and username and password. Copy down the user name and password. We will need these later in the workshop Leave all other settings at the default selections Click on Create  It will take approximately 5 - 10 minutes for your OpenSearch domain to be created. Upon successful creation you will see your OpenSearch domain status is active\nDo not proceed to the next step until you confirm that your domain status is active\nCreate an IAM Role  Go to the IAM Console Click on Roles  Click on Create role  Under the choose a use case click on Lambda and then click on Next:Permissions  Search for AdministratorAccess and click on the box next to the AdministratorAccess policies  Note for the purposes of this workshop we will give the IAM role AdministratorAccess. In a production environment you should scope down the permissions given to the IAM role\nClick on Next: Tags Click on Next: Review . There is no need to change anything on the Add tags page Name the role workshop-role Click on Create role  Back on the IAM Roles page search for and select the workshop-role you just created  On the summary page copy down the Role ARN. We will need this later in the workshop  Click on the Trust relationships tab Click on the Edit trust relationship  Replace the service section of the JSON with the following  \u0026quot;Service\u0026quot;: [\r\u0026quot;lambda.amazonaws.com\u0026quot;,\r\u0026quot;glue.amazonaws.com\u0026quot;\r]\rYour trust relationship policy document should look like the following\nClick on Update Trust Policy  Map IAM Role to OpenSearch Role  Go to the OpenSearch Console Click on the workshop-domain OpenSearch domain you created earlier Click on the OpenSearch Dashboard URL. This should open the URL in a web browser window  You will be prompted to log in. Using the user name and password you created during the OpenSearch deployment, log in If an additional pop up window is present after login asking about data upload click on Explore on my own If an additional pop up windows is present asking you to select your tenant select Global and click on Confirm  You should now see a window that looks like this\nClick on and expand the hamburger menu on the side bar of the OpenSearch home page Under the OpenSearch Plugins section click on Security  On the security page click on Roles from the left hand menu  On the roles page search for and click on all_access  On the all_access role page click on Mapped users Under the mapped users page click on Manage mapping  Under the backend roles section enter the ARN that you copied down earlier for the IAM workshop-role that you created Click on Map  Deploy Glue Jobs to Generate CloudWatch Logs CloudWatch provides log collection for AWS services. In order to generate logs that we can use in this workshop we will create two simple AWS Glue Jobs. We can run these jobs to produce logs. Subsequently we can analyze these logs in OpenSearch\n Go to the Glue Console On the left hand menu click on Jobs  On the jobs page click on Add job  Enter glue_job_success for the name of the job For the IAM tole select workshop-role Under the this job run section select A new script to be authored by you  Under the monitoring options click on Job metrics and Continuous logging  Under the security configuration, script libraries, and job parameters (optional) set the max concurrency to 10   Leave all other settings at the default selections and click on Next at the bottom of the page\n  On the connections page leave all of the setting at the defaults and click on Save job and edit script\n  In the edit job window copy and paste the following code into the editing window  from awsglue.context import GlueContext\rfrom pyspark.context import SparkContext\rsc = SparkContext()\rglueContext = GlueContext(sc)\rlogger = glueContext.get_logger()\rlogger.error(\u0026quot;Success!!\u0026quot;)\rClick on Save Click on Run Repeat steps 1 - 13 again. However this time name the Glue job glue_job_error and use the following code in the job  from awsglue.context import GlueContext\rfrom pyspark.context import SparkContext\rsc = SparkContext()\rglueContext = GlueContext(sc)\rlogger = glueContext.get_logger()\rlogger.error(\u0026quot;Error!!\u0026quot;)\rThe jobs will use the logger in AWS Glue to produce custom error log message Success!! and Error!!. Later in this workshop we will search for the log message in OpenSearch.\nYou have completed the set up for your AWS environment! When you are ready lets begin the next step Send Log Data to OpenSearch\n"
},
{
	"uri": "/open-search-alerting/2_environment_set_up.html",
	"title": "2. Environment Set Up",
	"tags": [],
	"description": "",
	"content": "We will need to deploy a few services and configure our AWS environment before we can get started.\nWe will need to complete the following set up steps:\n Create a Cloud9 environment Create an OpenSearch Domain Create a SNS Topic and Subscription Configure Identity Access Management Permission  To complete the environment set up with a CloudFormation template, follow the steps in the CloudFormation.\n"
},
{
	"uri": "/open-search-cross-cluster-replication/2_environment_set_up.html",
	"title": "2. Environment Set Up",
	"tags": [],
	"description": "",
	"content": "We will need to deploy a few services and configure our AWS environment before we can get started.\nWe will need to complete the following set up steps:\n Create a Cloud9 environment Create OpenSearch Domains (Leader and Follower) Configure Cross Cluster Replication  To complete the environment set up with a CloudFormation template, follow the steps in the CloudFormation.\n"
},
{
	"uri": "/open-search-fluentd/2_environment_set_up.html",
	"title": "2. Environment Set Up",
	"tags": [],
	"description": "",
	"content": "You will need to deploy a few services and configure our AWS environment before you can get started.\nYou will need to complete the following set up steps\n Create an OpenSearch Domain Create a Cloud9 development environment Configure Identity Access Management Permissions  Many parts of this deployment have been automated using a CloudFormation stack and additional post configuration scripts run from Cloud9\nTo begin the environment step up and configuration follow the steps in the CloudFormation (Automated)\n"
},
{
	"uri": "/open-search-log-analytics/2_environment_set_up.html",
	"title": "2. Environment Set Up",
	"tags": [],
	"description": "",
	"content": "We will need to deploy a few services and configure our AWS environment before we can get started.\nWe will need to complete the following set up steps\n Create an OpenSearch Domain Create a Kinesis Firehose Configure Identity Access Management Permissions  Two options to complete these steps are available. Option 1 uses a CloudFormation template to automate steps 1 + 2. Option 2 provides instructions to complete steps 1 + 2 manually via. the AWS console.\nTo complete the environment step up via. a CloudFormation template follow the steps in the CloudFormation (Automated)\nOf if you would prefer to set up your AWS environment manually via. the AWS console follow the steps in the Console Deploy (Manual)\n"
},
{
	"uri": "/open-search-fluentd/3_configure_fluentd.html",
	"title": "3. Configure Fluentd",
	"tags": [],
	"description": "",
	"content": "Step 1 - Install Fluentd You need to install Fluentd in the Cloud9 environment. The Cloud9 environment is based on an Ubuntu linux EC2 machine. you will follow the instructions for installing Fluentd on Ubuntu\nIn the Cloud9 terminal execute the following commands\n gem install fluentd --no-doc gem install fluent-plugin-opensearch  Step 2 - Setup Fluentd After installing Fluentd to create a new project execute the following command on the Cloud9 terminal\n fluentd --setup ./fluent  This will create a new folder in Cloud9 with a fluent.conf file. you will edit the fluent.conf file to configure Fluentd\nOpen the flunt.conf file in Cloud9  Delete all of the code in the file and replace it with the code below  \u0026lt;source\u0026gt;\r@type tail\rpath /var/log/apache2/access.log\rpos_file /var/log/td-agent/apache2.access_log.pos\rtag os.apache.access\rformat apache2\r\u0026lt;/source\u0026gt;\r\u0026lt;match match os.*.*\u0026gt;\r@type opensearch\rinclude_timestamp true\rindex_name fluentd\r\u0026lt;endpoint\u0026gt;\rurl \u0026lt;opensearch_domain_endpoint\u0026gt;\rregion \u0026lt;aws_region\u0026gt;\raccess_key_id \u0026lt;your_aws_id\u0026gt;\rsecret_access_key \u0026lt;your_aws_security_key\u0026gt;\r\u0026lt;/endpoint\u0026gt;\r\u0026lt;/match\u0026gt;\rEdit to the new fluent.conf file replace the following\nReplace \u0026lt;opensearch_domain_endpoint\u0026gt; with the value of the OSDomainURL key from the CloudFormation stack outputs Replace \u0026lt;aws_region\u0026gt; with the value of the Region key from the CloudFormation stack outputs Replace \u0026lt;your_aws_id\u0026gt; with the value of the AccessKeyId key from the CloudFormation stack outputs Replace \u0026lt;your_aws_security_key\u0026gt; with the value of the SecretAccessKey key from the CloudFormation stack outputs  Save the fluent.conf file  Step 3 - Run Fluentd Now that you have installed and configured Fluentd (via. updating the fluent.conf file) it is time to run Fluentd. To run Fluentd\nBefore you can run Fluentd you need to open permissions on two folders within the EC2 instance the Cloud9 runs on. Run the following commands in the Cloud9 terminal\n sudo mkdir /var/log/apache2/ sudo chmod 777 /var/log/apache2/ sudo mkdir /var/log/td-agent/ sudo chmod 777 /var/log/td-agent/  Note: you may get a \u0026lsquo;cannot create directory: file exists error\u0026rsquo;. If this occurs, you can ignore and continue\n\rYou can now start Fluntd by running the following command in the Cloud9 console\nfluentd -c ./fluent/fluent.conf -vv\nThe -vv flag enables verbose logging. This allows us to more easily keep track on Fluentd\u0026rsquo;s actions.\nStep 4 - Create Sample Apache Log Fluentd is watching the access.log file. By default this file has no log data for Fluentd to parse and send to OpenSearch.\nTo generate sample log data for Fluentd to process\n Open a new terminal window in Cloud9 by clicking on the plus symbol next to the current terminal and selecting New Terminal  In the new terminal execute the following command  ab -n 10000 -c 10 http://localhost/\nRunning this command will generate 10000 sample logs in the access.log file Fluentd is monitoring\nFluentd will recognize the new entries have been added to the access.log file and will parse, send the log data to OpenSearch.\nYou can now move on the next step Search Log to view and search the logs Fluentd sent to OpenSearch.\n"
},
{
	"uri": "/anomaly-detection-w-msk/3_configure_lambda.html",
	"title": "3. Configure Lambda",
	"tags": [],
	"description": "",
	"content": "You need to configure a Lambda function to read data from MSK and write it to OpenSearch. To configure the Lambda function\n Navigate to the Lambda console Click on msk-os-lambda  Click on Add trigger  Select MSK from the trigger drop down Select msk-workshop-cluster from the MSK cluster drop down Enter 500 for the batch size Enter 30 for the batch window Enter ApplicationMetricTopic for the topic name Select Latest from the starting position drop down Click on Add  You will automatically be redirected to a page that looks like the one below. Take note of the status of trigger. Remain on this page refreshing as necessary until the trigger status is Enabled\nClick on the Code tab Update the os_url variable with the value of the OSDomainURL key from the CloudFormation stack output  Click on the Deploy button  Our Lambda function is now set up. When you are ready begin the next step Send Logs\n"
},
{
	"uri": "/open-search-alerting/3_create_alert.html",
	"title": "3. Create Alert",
	"tags": [],
	"description": "",
	"content": "Now you will configure an OpenSearch Alert using the index you created in the last step. First you need to configure the SNS Topic created from the CloudFormation script as an OpenSearch Alert Destination. This will allow OpenSearch to use the SNS destination to send alerts.\nStep 1 - Create a Destination  Click on the top left hand menu on the OpenSearch dashboard Click on Alerting under the OpenSearch Plugins section  On the alerting page click on the Destinations Click on Add destination  On the add destination page configure the destination entering the values for each field in the table below\n   Field Value     Name SNS Destination   Type Amazon SNS   SNS topic ARN TopicARN value from CloudFormation output   IAM role ARN IAMRoleARN value from CloudFormation output    An example destination may look like\nOnce you finish configuring the destination click on Create  Step 2 - Set up a Monitor Using the destination you configured, you will configure a monitor that will trigger an alert if cpu_util exceeds 50.\n On the OpenSearch alerting page, click on the Monitors tab Click on Create monitor  Fill in the Monitor details and Data source sections with the following values     Field Value     Monitor name High CPU Alert   Monitor type Per query monitor   Monitor defining method Visual editor   Frequency By interval   Run every 1 Minutes   Index alert-1   Time field eventtime    An example monitor details and data source may look like\nUnder the query section, click + Add metric  Select max() as the aggregation function Select cpu_util as the field Click on Save  Under the triggers section, click Add trigger Fill in the trigger sections with the following values     Field Value     Trigger name CPU over 50   Severity level 1 (Highest)   Trigger condition IS ABOVE 50    An example trigger configuration may look like\nFor the trigger, configure an Action to send an email to our SNS Topic, fill in the action sections with the following values     Field Value     Action name Email Alert   Destination SNS Destination (Amazon SNS)   Message subject High CPU!   Message Leave default message    An example action may look like\nClick on Create  After the Monitor saves, your new High CPU Alert monitor dashboard will be available\nWhen you are ready move on, go to the next step Trigger Alert\n"
},
{
	"uri": "/open-search-fluentd/",
	"title": "3. Fluntd Log Ingestion",
	"tags": [],
	"description": "",
	"content": "Fluentd Log Ingestion This workshop is an introduction to ingesting logs with Fluentd. Fluentd is an open source data collector. Fluentd supports log parsing and ingestion directly to Amazon OpenSearch service.\nIn this workshop you will implement the following architecture\nTo implement this architecture you install Fluentd in an Ubuntu Cloud9 environments. Once installed you configure Fluentd to parse and send apache2 access logs to an OpenSearch domain.\nWhen you are ready to begin the lab navigate to Getting started\n"
},
{
	"uri": "/open-search-log-analytics/3_send_log_data_to_kinesis_fire_hose.html",
	"title": "3. Send Log Data to Kinesis Firehose",
	"tags": [],
	"description": "",
	"content": "We need to send sample log data to Kinesis Data Firehose which in turn will send the data to OpenSearch.\nWe will run a python application that will send sample Spark log data to OpenSearch. We will run the sample Python application in a Cloud9 environment.\nIf in the previous section Environment Set Up you chose the CloudFormation (Automated) deployment option your Cloud9 environment is already created. Navigate to the Cloud9 Console and click on Open IDE under the already created workshop-cloud9 environment. Then skip the Create a Cloud9 environment section below and start at the Run a Python Application from Cloud section. If you did not use the CloudFormation template in the Environment Set Up and instead used the Console Deploy (Manual) complete the Create a Cloud9 environment section below before starting on the Run a Python Application from Cloud section.\n\rCreate a Cloud9 environment  Go to the Cloud9 Console Click on Create environment  Under the name environment section enter workshop-cloud9 for the name Click on Next step Leave all of the settings at the default selections Click on Next step Click on Create environment  After the Cloud9 environment is created your browser will automatically be redirected to the Cloud9 console\nRun a Python Application from Cloud9 Within the Cloud9 console running the following commands in the console section of the Cloud9 environment\n wget https://sharkech-public.s3.amazonaws.com/opensearch-log-analytics/data-producer/Log_Producer_Desktop.py  The image below highlights were to run the commands. Run all of the commands in order\nwget https://sharkech-public.s3.amazonaws.com/opensearch-log-analytics/data-producer/sample_logs/spark.txt pip install boto3 python Log_Producer_Desktop.py  These commands download the sample log data. They also download and configure the python script that will send the sample log data to Kinesis Data Firehose.\nUpon running the last command you should be messages appearing in the Cloud9 console indicating the logs are being sent.\nLeave this browser window open. This way the python application continues to run and send data.\nWhen you are ready move on to the next step Visualize and Analyze\n"
},
{
	"uri": "/collect-log-cloud-watch/3_send_log_data_to_opensearch.html",
	"title": "3. Send Log Data to OpenSearch",
	"tags": [],
	"description": "",
	"content": "We have now created an OpenSearch domain, configured the required IAM and OpenSearch permissions and we created two Glue jobs that we can run to produce logs.\nAll of the log information for the Glue jobs is collected in CloudWatch. Lets set up Lambda functions that will – in real time – send the CloudWatch log data to OpenSearch\nWe can complete this in the following steps\n Set up CloudWatch to OpenSearch Lambda Function(s) Re-run the Glue Jobs to Create additional CloudWatch Logs  Set up CloudWatch to OpenSearch Lambda Function(s)  Navigate to the CloudWatch Console On the left hand menu click on Log groups  On the log groups page of CloudWatch you should see a few log groups which begin with /aws-glue/\nClick on the log group aws-glue/jobs/error Click on the Subscription filters tab Click on the Create drop down Click on the Create Amazon OpenSearch Service subscription filter  On the Create Amazon OpenSearch Service subscription filter page, under the choose destination click on This account From the drop down select the workshop-domain  Under the Lambda Function section select the IAM role workshop-role that we created earlier  Under the configure log format and filters select JSON For the subscription filter pattern enter \u0026quot; \u0026quot; For the subscription filter name all log  Click on Start streaming  The steps you just completed created a lambda function that will send CloudWatch logs to OpenSearch in real time.\nNow that we have completed this process for the \t/aws-glue/jobs/error log group Navigate back to the CloudWatch Console and repeat steps 1 - 13 for log groups \t/aws-glue/jobs/logs-v2 and /aws-glue/jobs/output\nRe-run the Glue Jobs to Create additional CloudWatch Logs  Go to the Glue Console On the left hand menu click on Jobs  Click on the check box next to the glue_job_success job Click on the Action drop down Click on the Run job button  Repeat step 3 - 4 for of the jobs you created earlier ie. glue_job_success and glue_job_error. Run each job a few times. The goal is to generate some CloudWatch logs that will be sent to OpenSearch.\nYou can view the job run history for a job by click on the check box next to job name and viewing the history tab\nEnsure that you have at least 3 runs completed for each job. The job status for the glue_job_success should be Succeeded and the job status for the glue_job_error should be Failed\nWhen you have at least 3 completed job runs for each job begin the next step Search Logs\n"
},
{
	"uri": "/open-search-cross-cluster-replication/3_setup_connection.html",
	"title": "3. Setup Cross-Cluster Connection",
	"tags": [],
	"description": "",
	"content": "Now you will create a cross-cluster connection between our two OpenSearch domains. This will permit indexes to replicate between them.\nStep 1 - Request Outbound Connection  Open the AWS Console and navigate to the Domains section of OpenSearch  Select the \u0026lsquo;workshop-domain-follower\u0026rsquo; domain and open the Connections tab. Click Request under the Outbound connection section.  Here we will request a connection with the \u0026lsquo;workshop-domain-leader\u0026rsquo; domain. Fill out the Connection alias, connect to a cluster in this AWS account, and select the \u0026lsquo;workshop-domain-leader\u0026rsquo; domain in the dropdown. Click Request.  Step 2 - Approve Connection Now that you requested a connecton from the follower domain to the leader domain, the leader domain must approve the connection.\n  Under the Domains section of OpenSearch, open the \u0026lsquo;workshop-domain-leader\u0026rsquo; domain and open the Connections tab.\n  In the Inbound connections section, select the request from \u0026lsquo;workshop-domain-follower\u0026rsquo; and click Approve. A popup will ask for your confirmation.\n  Now the domains are ready for replication.\nWhen you are ready move on, go to the next step Test Replication\n"
},
{
	"uri": "/anomaly-detection-w-msk/",
	"title": "4. Anomaly Detection with MSK",
	"tags": [],
	"description": "",
	"content": "Anomaly Detection with Logs from Apache Kafka This workshop covers how to use the built in anomaly detection capabilities of Amazon OpenSearch service. Additionally it covers a method of ingesting real time data from Amazon Managed Streaming for Apache Kafka (MSK) into Amazon OpenSearch service.\nIn this workshop you will implement the following architecture\nThe architecture uses a python script run in a Cloud9 environment to simulate devices creating log data. The data is sent in real time to a MSK cluster. A Lambda function reads the log data sent to MSK and ingests the data into OpenSearch. Once the OpenSearch index is created and populated with data we use the anomaly detection capabilities of OpenSearch to run a historic analysis and detected any anomalies that may have occurred.\nWhen you are ready to begin the lab navigate to Getting started\n"
},
{
	"uri": "/collect-log-cloud-watch/4_search_logs.html",
	"title": "4. Search Logs",
	"tags": [],
	"description": "",
	"content": "Now that we are sending logs from CloudWatch to OpenSearch lets create an index patterns and run a simple search to validate that our logs are actually being sent to OpenSearch.\nOpenSearch is capable of more than search it can also build visualizations and more. In this section we will perform a simple search to ensure that our logs are actually being delivered.\nCheck out the Visualize and Analyze section of the OpenSearch Log Analytics workshop for a more thorough lab on searching and visualizing logs.\nOpen the OpenSearch Dashboard  Go to the OpenSearch Console Click on the workshop-domain OpenSearch domain you created earlier  Click on the OpenSearch Dashboard URL. This should open the URL in a web browser window  You will be prompted to log in. Using the user name and password you created during the OpenSearch deployment, log in If an additional pop up window is present after login asking about data upload click on Explore on my own If an additional pop up windows is present asking you to select your tenant select Global and click on Confirm  You should now see a window that looks like this\nCreate an Index Pattern The Lambda functions the send the messages from CloudWatch to OpenSearch will create a new OpenSearch index each day. Each index name will start with cwl and will be followed by the date.\nTo search all of the CloudWatch logs (ie. multiple days) we will create an index pattern in OpenSearch. The index pattern will be a representation of all of the cwl log indexes for all of the days.\n In the OpenSearch Dashboard, expand the side menu and click on Stack Management under management section  On the stack management page click on Index Patterns on the left hand menu  On the index patterns page click on Create index pattern  Enter cwl-* under the index pattern name section Click on Next step  Select @timestamp as the primary time field Click on Create index pattern  We have now created an index pattern! We can use the index pattern to analyze our logs\nSearch the Logs OpenSearch provides the ability to easily search log data. Lets run a simple search on our logs to validate that they have been successfully sent from CloudWatch to OpenSearch.\nThe Glue Jobs that you ran earlier logged custom message of Success!! and Error!!\nWe can search for the Success!! logs\n In the OpenSearch Dashboard expand the side menu and click on Discover under the OpenSearch Dashboards section  This will bring you to the discovery page. On this page we can see the log data sent from CloudWatch\nEnsure that you adjust the time range in the top right hand corner to include a large enough range that all of the logs we collected are included. Click Update or Refresh once you update the time range  We can now search our CloudWatch logs. Try searching for Success!! you will see the log message that the Glue job created during its execution  You will see a few logs that contain the customer log message from the Glue Job. Feel free to spend a few minutes trying other OpenSearch searches. See if you can search for the other Error!! logs\nWhen you are ready proceed to the next step Clean Up if you want to delete the resources we used for this workshop\n"
},
{
	"uri": "/open-search-fluentd/4_search_logs.html",
	"title": "4. Search Logs",
	"tags": [],
	"description": "",
	"content": "OpenSearch provides us the ability to analyze out logs. Let\u0026rsquo;s begin by navigating to the OpenSearch Dashboard\nStep 1 - Open the OpenSearch Dashboard  Go to the OpenSearch Console Click on the fluentd-domain OpenSearch domain you created earlier  Click on the OpenSearch Dashboard URL. This should open the URL in a web browser window  You will be prompted to log in. For the user name enter OSMasterUser for the password enter AwS#OpenSearch1 If an additional popup window is present after login asking about data upload click on Explore on my own If an additional popup window is present asking you to select your tenant select Global and click on Confirm  You should now see a window that looks like this\nStep 2 - Create an Index Pattern In order to search our logs via. the OpenSearch dashboard you need to create an index pattern. Follow the steps below to create an index pattern for the Fluentd logs\n In the OpenSearch Dashboard, expand the side menu and click on Stack Management under management section  On the stack management page click on Index Patterns on the left hand menu  On the index patterns page click on Create index pattern  Enter fluentd* under the index pattern name section  Click on Next step Click on the time field drop down and select @timestamp  Click on Create index pattern  You have now created an index pattern! You can use the index pattern to analyze our logs\nStep 3 - Search the Logs OpenSearch provides the ability to easily search log data. Let\u0026rsquo;s view and search our logs sent by Fluentd\n In the OpenSearch Dashboard expand the side menu and click on Discover under the OpenSearch Dashboards section  Expand the time range that OpenSearch will view to the Last 7 days  You can now see the logs sent to OpenSearch from Fluentd\nYou can expand any of the logs to view all of the available fields\nYou can also search logs using the search bar at the top of the page\nWhen you are ready proceed to the next step Clean Up if you want to delete the resources you used for this workshop\n"
},
{
	"uri": "/anomaly-detection-w-msk/4_send_logs.html",
	"title": "4. Send Logs",
	"tags": [],
	"description": "",
	"content": "You need to send log data to MSK so it can subsequently be sent to OpenSearch (in real time) by the Lambda function we deployed.\nYou will send the log data to Kafka via. a Python Script run from Cloud9\n Navigate to the MSK page in the AWS console Select the msk-workshop-cluster  Click on View client information  Copy the plaintext bootstrap servers connection information Navigate to the Cloud9 console Click on Open IDE  In the Cloud9 environment navigate and open Kafka_OpenSearch_Anomaly_Detection/Kafka/2_base_data.py Update the \u0026lt;plain_text_bootstrap_server\u0026gt; with the plaintext bootstrap servers connection information you copied down and save the file Run pip install pykafka Run cd ~ in the Cloud9 terminal Run python environment/Kafka_OpenSearch_Anomaly_Detection/Kafka/2_base_data.py  Allow the python script to run until the terminal looks similar to the image below\nOpen Kafka_OpenSearch_Anomaly_Detection/Kafka/3_anomoly_data.py Update the \u0026lt;plain_text_bootstrap_server\u0026gt; with the plaintext bootstrap servers connection information you copied down and save the file Run python environment/Kafka_OpenSearch_Anomaly_Detection/Kafka/3_anomoly_data.py  Allow this python script to run until completion. This may take several minutes. If you leave the Cloud9 environment open in your web browser as a separate tab you can move on to the next steps\nWhen the script is complete the Cloud9 terminal will look similar to the image below\nWe have completed the process of sending data to MSK. When you are ready begin the next step Anomaly Detection\n"
},
{
	"uri": "/open-search-cross-cluster-replication/4_test_replication.html",
	"title": "4. Test Replication",
	"tags": [],
	"description": "",
	"content": "You will now create an index and replication rule that will copy data from the leader to the follower domain.\nEnvironment Info  Goto the OpenSearch service page and note the Domain endpoints for both leader and follower domain.   Be sure that the URL does not contain a following forward slash.\n\rStep 1 - Run a Python Application from Cloud9  Navigate to the Cloud9 Console Click on Open IDE under the already created workshop-cloud9 environment.  The OpenSearch_API_Examples repository will be automatically pulled down to your environment. Do the following within the Cloud9 console.\nOpen the file OpenSearch_API_Examples/Cross_Cluster_Replication/cross_cluster_replication.py Update the leader_os_url and follower_os_url variables to your OpenSearch domains. Goto File -\u0026gt; Save to save the changes to cross_cluster_replication.py  In the terminal window, run the following commands:\npip install requests python OpenSearch_API_Examples/Cross_Cluster_Replication/cross_cluster_replication.py  The image below highlights the commands.\nThe script creates and index called log-data-1, inserts a sample document, and creates a replication rule for all indexes that start with \u0026lsquo;log\u0026rsquo;.\nStep 2 - Verify Replication  Goto the OpenSearch service page and note the Domain endpoints for both leader and follower domain.   Open the Leader OpenSearch Dashboard. Once you open the dashboard URL you will be prompted to login.\n  Enter OSMasterUser as the username\n  Enter AwS#OpenSearch1 as the password\n  Click on Log In\n  If an additional pop up window is present after login asking about data upload click on Explore on my own\n  If an additional pop up windows is present asking you to select your tenant select Global and click on Confirm\n  Once you have logged in to the OpenSearch dashboard\nClick on the top left hand menu on the OpenSearch dashboard Click on Query Workbench under the OpenSearch Plugins section  Select the data from log-data-1 using this simple query: SELECT * FROM log-data-1  Follow the same procedure for the Follower domain.  Observe that the result sets match between Leader and Follower.\nSend more data by re-running the Python script and observe the result sets. python OpenSearch_API_Examples/Cross_Cluster_Replication/cross_cluster_replication.py  When you are ready, proceed to the next step Clean Up if you want to delete the resources you used for this workshop.\n"
},
{
	"uri": "/open-search-alerting/4_trigger_alert.html",
	"title": "4. Trigger Alert",
	"tags": [],
	"description": "",
	"content": "You will now simulate an alert condition and verify that our alert triggers and you receive an email\nStep 1 - Run a Python Application from Cloud9  Navigate to the Cloud9 Console Click on Open IDE under the already created workshop-cloud9 environment.  The OpenSearch_API_Examples repository will be automatically pulled down to your environment. Do the following within the Cloud9 console.\n Open the file OpenSearch_API_Examples/Alerting/2_trigger_alert.py Update the os_url variable to your OpenSearch instance. This is the value of the OSDomainURL key from the CloudFormation stack output. Goto File -\u0026gt; Save to save the changes to 2_trigger_alert.py  In the terminal window, run the following commands:\npython OpenSearch_API_Examples/Alerting/2_trigger_alert.py  The image below highlights the commands.\nThe script will to send high CPU values to OpenSearch until you stop the script. You can stop it with Ctrl-C in the terminal window.\nStep 2 - Verify Alerting  Emails should have been received in the specified email address.  Click on the top left hand menu on the OpenSearch dashboard Click on Alerting under the OpenSearch Plugins section  Click on Monitors Click on the High CPU Alert  Observe the History section. You should see the Triggered status occurring.\nUnder the Alerts section, click on the alert and acknowledge the alert to stop receiving email.  When you are ready proceed to the next step Clean Up if you want to delete the resources you used for this workshop\n"
},
{
	"uri": "/open-search-log-analytics/4_visualize_analyze.html",
	"title": "4. Visualize and Analyze",
	"tags": [],
	"description": "",
	"content": "OpenSearch provides us the ability to analyze out logs. Let\u0026rsquo;s begin by navigating to the OpenSearch Dashboard\nOpen the OpenSearch Dashboard  Go to the OpenSearch Console Click on the workshop-domain OpenSearch domain you created earlier  Click on the OpenSearch Dashboard URL. This should open the URL in a web browser window  You will be prompted to log in. For the user name enter OSMasterUser for the password enter AwS#OpenSearch1 If an additional popup window is present after login asking about data upload click on Explore on my own If an additional popup windows is present asking you to select your tenant select Global and click on Confirm  You should now see a window that looks like this\nCreate an Index Pattern When we deployed Kinesis Data Firehose we configured it to create a new index in OpenSearch every 1 hr. We also configured it to name each index starting with workshop-log\nThis means that open search will have 1 index for each hour it is sent logs, and that these indices' names will start with workshop-log\nIn order to work with all of the logs (ie. multiple hours) we will create an index pattern in OpenSearch. The index pattern will be a representation of all of the workshop-log indexes for all of the hours\n In the OpenSearch Dashboard, expand the side menu and click on Stack Management under management section  On the stack management page click on Index Patterns on the left hand menu  On the index patterns page click on Create index pattern  Enter workshop-* under the index pattern name section Click on Next step  Select date as the time field Click on Create index pattern  We have now created an index pattern! We can use the index pattern to analyze our logs\nSearch the Logs OpenSearch provides the ability to easily search log data. Let\u0026rsquo;s run a few simple searches on our logs\n In the OpenSearch Dashboard expand the side menu and click on Discover under the OpenSearch Dashboards section  This will bring you to the discovery page. By default this view of the log data views the last 15 minutes. Let\u0026rsquo;s adjust it to display the last two years of data.\nClick on date/time filter next to the search bar Configure the relative date range for 2 years Click on the Update  We can now see the log data we sent via. the Cloud9 Python application\nWe can now run a few different searched against our index pattern. To get started lets look for any log messages that related to spark broadcast operations.\nRun the search \u0026quot;spark\u0026quot; AND \u0026quot;broadcast\u0026quot;  OpenSearch displays the 74 logs of the total 2000 logs. Now that you have run a search. Try running at least 3 other searches. A few search suggestions are below. However feel free to come up with you own\nSuggested searches\n date:2021-01-01 date:2021-01-01 AND message:Memory date\u0026lt;2021-01-01  After you have run a few searches. We can look at creating a visualization and dashboard\nCreate a Visualization  In the OpenSearch Dashboard expand the side menu and click on Visualize under the OpenSearch Dashboards section  Click on Create new visualization  Click on Gauge from the list of visualizations Click workshop- from the list of index patterns Click on date/time filter next to the search bar Configure the relative date range for 2 years Click on the Update  On the create page enter \u0026quot;spark\u0026quot; AND \u0026quot;broadcast\u0026quot; in the search bar  This will produce a gauge chart visual.\nClick on the Save button. Name the visual anything you would like  Create a Dashboard Dashboards allow you to combine multiple visualizations in a single place. Let\u0026rsquo;s build a simple dashboard\n In the OpenSearch Dashboard expand the side menu and click on Dashboards under the OpenSearch Dashboards section  On the dashboard page click on Create new dashboard  On the dashboard page click on Add an existing  Select the visualization you just created.  Use the create new button or repeat the earlier process to create at least 2 additional visuals. Try to create visuals that can show\n Horizontal bar chart showing how many logs we received per day Bar chart showing the number of logs level (ie. ERROR, WARNING, INFO)  When you are ready proceed to the next step Clean Up if you want to delete the resources we used for this workshop\n"
},
{
	"uri": "/anomaly-detection-w-msk/5_anomaly_detection.html",
	"title": "5. Anomaly Detection",
	"tags": [],
	"description": "",
	"content": "Now that you have sent data to MSK via. the two python scripts in Cloud9. Lets first validate that we can see the log data in OpenSearch. Then second, lets create our anomaly detector.\nStep 1 - Create Index Pattern  Navigate to the OpenSearch page in the AWS console Click on workshop-domain  Click on the OpenSearch Dashboards URL  You will be prompted to log in. For the user name enter OSMasterUser for the password enter AwS#OpenSearch1 If an additional popup window is present after login asking about data upload click on Explore on my own If an additional popup window is present asking you to select your tenant select Global and click on Confirm In the OpenSearch Dashboard, expand the side menu and click on Stack Management under management section  On the stack management page click on Index Patterns on the left hand menu  On the index patterns page click on Create index pattern  Enter infa-logs-* under the index pattern name section  Click on Next step Click on the time field drop down and select eventtime  Click on Create index pattern  You have now created an index pattern! You can use the index pattern to search our log and validate that the logs have been send from MSK to OpenSearch\nStep 2 - Search Logs  In the OpenSearch Dashboard expand the side menu and click on Discover under the OpenSearch Dashboards section  Expand the time range that OpenSearch will view to the Last 3 year  You will now be able to see logs from the past 3 yrs\nStep 3 - Create Anomoly Detector  In the OpenSearch Dashboard expand the side menu and click on Anomaly detection under the OpenSearch Plugins section  Click on Create detector  Enter cpu_detector for the name of the detector Pick infa-logs-1 for the data source of the detector  Pick eventtime for the timestamp of the detector Change the detector interval to 1440 Click on Next  Enter CPU-Utilization for the feature name Select average() for the aggregation method Select cpu_util for the field  Click the check box to enable categorical fields Select application_id for the categorical field Click on Next  Click to uncheck the box to start real-time detector automatically Click the check box to run historical analysis detection Adjust the historical analysis date range to the last 3 yrs Click on Apply Click on Next  Click on Create detector  Step 4 - View Anomaly Detector Results  Click on the Historical analysis tab  You will now be able to see a heat map of the anomaly\u0026rsquo;s detected by application. Click on an of the rectangles in the heat map to see a more detailed view of the anomaly\nWhen you are ready proceed to the next step Clean Up if you want to delete the resources we used for this workshop\n"
},
{
	"uri": "/collect-log-cloud-watch/5_clean_up.html",
	"title": "5. Clean Up",
	"tags": [],
	"description": "",
	"content": "Delete OpenSearch  Go to the OpenSearch Console Click on the workshop-domain OpenSearch domain you created earlier  Click on Delete  Follow the instructions in the prompt to complete the delete  Delete Glue Jobs  Go to the Glue Console On the left hand menu click on Jobs  Click on the box next to a Glue Job Click on the Actions drop down From the drop down click on Delete  In the pop up window click on the Delete button Repeat this process until you have deleted both of the Glue Jobs (glue_job_success and glue_job_error) the we created earlier  Delete Cloud Watch Log Groups  Navigate to the CloudWatch Console On the left hand menu click on Log groups  On the log groups page click on the check box to select all of the log groups Click on the Actionsdrop down Click on Delete log group(s)  Delete Lambda Functions  Navigate to the Lambda Function Console Click on Functions on the left hand menu  Click on the check box next to the function Click on the actions drop down Select delete  Click on the delete button in the pop up window Repeat the process until you have deleted the Lambda Functions that you want to delete  You have completed deleting the resources used in this workshop\n"
},
{
	"uri": "/open-search-alerting/5_clean_up.html",
	"title": "5. Clean Up",
	"tags": [],
	"description": "",
	"content": "Delete CloudFormation Stack  Navigate to the CloudFormation console Select the stack that was deployed earlier in the lab Click on Delete  The CloudFormation stack will take 5 - 10 minutes to delete\n"
},
{
	"uri": "/open-search-cross-cluster-replication/5_clean_up.html",
	"title": "5. Clean Up",
	"tags": [],
	"description": "",
	"content": "Delete CloudFormation Stack  Navigate to the CloudFormation console Select the stack that was deployed earlier in the lab Click on Delete  The CloudFormation stack will take 5 - 10 minutes to delete\n"
},
{
	"uri": "/open-search-fluentd/5_clean_up.html",
	"title": "5. Clean Up",
	"tags": [],
	"description": "",
	"content": "Delete CloudFormation Stack  Navigate to the CloudFormation console Select the stack that was deployed earlier in the lab Click on Delete  The CloudFormation stack will take 5 - 10 minutes to delete\n"
},
{
	"uri": "/open-search-log-analytics/5_clean_up.html",
	"title": "5. Clean Up",
	"tags": [],
	"description": "",
	"content": "Delete Cloud 9 If you used CloudFormation to deploy your resources you can navigate to the CloudFormation console select the stack named open-search-deployment and select delete.\nIf you did not use the CloudFormation to deploy your resources follow the steps below to manually delete each resource.\nIf you did not use CloudFormation to deploy your resources you can manually delete the resources following the steps below\n Go to the Cloud9 Console  Select your Cloud9 environment Click on Delete Follow the instructions in the prompt to complete the delete  Delete Kinesis Data Firehose  Go to the Kinesis Firehose Console Click on the selector next to the Kinesis Data Firehose we created earlier  Click on Delete Follow the instructions in the prompt to complete the delete  Delete OpenSearch  Go to the OpenSearch Console Click on the workshop-domain OpenSearch domain you created earlier  Click on Delete  Follow the instructions in the prompt to complete the delete  You have completed deleting the resources used in this workshop\n"
},
{
	"uri": "/anomaly-detection-w-msk/6_clean_up.html",
	"title": "6. Clean Up",
	"tags": [],
	"description": "",
	"content": "Delete CloudFormation Stack  Navigate to the CloudFormation console Select the stack that was deployed earlier in the lab Click on Delete  The CloudFormation stack will take 5 - 10 minutes to delete\n"
},
{
	"uri": "/anomaly-detection-w-msk/1_getting_started/aws_event.html",
	"title": "AWS Event",
	"tags": [],
	"description": "",
	"content": "\rOnly complete this section if you are at an AWS hosted event (such as re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee). If you are running the workshop on your own, go to Self Paced.\n\rLog into AWS Console via. AWS Workshop Portal Your instructor has already created an AWS account for you. Your instructor should provide you a participant hash.\nOnce you have your participant hash go to https://dashboard.eventengine.run/\nEnter your participant has and select Accept Terms \u0026amp; Login\nSelect how you want to log in. Selecting Email One-Time Password (OTP) is recommended. However, you can also use your Amazon.com retail account to login.\nFollow the prompts to complete the login process. Once you have successfully logged in. You will see the following screen. Select AWS Console\nA window will open. From the window select Open AWS Console\nThis will open the AWS Console in a new window on your web browser. If you can see the home page for the AWS Console as depicted below you have successfully logged into your AWS account.\nNow that you have successfully logged into your AWS account and are able to access the AWS Console, let\u0026rsquo;s begin the next step Environment Set Up\n"
},
{
	"uri": "/collect-log-cloud-watch/1_getting_started/aws_event.html",
	"title": "AWS Event",
	"tags": [],
	"description": "",
	"content": "\rOnly complete this section if you are at an AWS hosted event (such as re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee). If you are running the workshop on your own, go to Self Paced.\n\rLog into AWS Console via. AWS Workshop Portal Your instructor has already created an AWS account for you. Your instructor should provide you a participant hash.\nOnce you have your participant hash go to https://dashboard.eventengine.run/\nEnter your participant has and select Accept Terms \u0026amp; Login\nSelect how you want to log in. Selecting Email One-Time Password (OTP) is recommended. However you can also use your Amazon.com retail account to login.\nFollow the prompts to complete the login process. Once you have sucssfully loged in. You will see the following screen. Select AWS Console\nA window will open. From the window select Open AWS Console\nThis will open the AWS Console in a new window on your web browser. If you can see the home page for the AWS Console as depicted below you have sucssfully logged into your AWS account.\nNow that you have successfully logged into your AWS account and are able to access the AWS Console, lets begin the next step Environment Set Up\n"
},
{
	"uri": "/open-search-alerting/1_getting_started/aws_event.html",
	"title": "AWS Event",
	"tags": [],
	"description": "",
	"content": "\rOnly complete this section if you are at an AWS hosted event (such as re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee). If you are running the workshop on your own, go to Self Paced.\n\rLog into AWS Console via. AWS Workshop Portal Your instructor has already created an AWS account for you. Your instructor should provide you a participant hash.\nOnce you have your participant hash go to https://dashboard.eventengine.run/\nEnter your participant has and select Accept Terms \u0026amp; Login\nSelect how you want to log in. Selecting Email One-Time Password (OTP) is recommended. However, you can also use your Amazon.com retail account to login.\nFollow the prompts to complete the login process. Once you have successfully logged in. You will see the following screen. Select AWS Console\nA window will open. From the window select Open AWS Console\nThis will open the AWS Console in a new window on your web browser. If you can see the home page for the AWS Console as depicted below you have successfully logged into your AWS account.\nNow that you have successfully logged into your AWS account and are able to access the AWS Console, let\u0026rsquo;s begin the next step Environment Set Up\n"
},
{
	"uri": "/open-search-cross-cluster-replication/1_getting_started/aws_event.html",
	"title": "AWS Event",
	"tags": [],
	"description": "",
	"content": "\rOnly complete this section if you are at an AWS hosted event (such as re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee). If you are running the workshop on your own, go to Self Paced.\n\rLog into AWS Console via. AWS Workshop Portal Your instructor has already created an AWS account for you. Your instructor should provide you a participant hash.\nOnce you have your participant hash go to https://dashboard.eventengine.run/\nEnter your participant has and select Accept Terms \u0026amp; Login\nSelect how you want to log in. Selecting Email One-Time Password (OTP) is recommended. However, you can also use your Amazon.com retail account to login.\nFollow the prompts to complete the login process. Once you have successfully logged in. You will see the following screen. Select AWS Console\nA window will open. From the window select Open AWS Console\nThis will open the AWS Console in a new window on your web browser. If you can see the home page for the AWS Console as depicted below you have successfully logged into your AWS account.\nNow that you have successfully logged into your AWS account and are able to access the AWS Console, let\u0026rsquo;s begin the next step Environment Set Up\n"
},
{
	"uri": "/open-search-fluentd/1_getting_started/aws_event.html",
	"title": "AWS Event",
	"tags": [],
	"description": "",
	"content": "\rOnly complete this section if you are at an AWS hosted event (such as re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee). If you are running the workshop on your own, go to Self Paced.\n\rLog into AWS Console via. AWS Workshop Portal Your instructor has already created an AWS account for you. Your instructor should provide you a participant hash.\nOnce you have your participant hash go to https://dashboard.eventengine.run/\nEnter your participant has and select Accept Terms \u0026amp; Login\nSelect how you want to log in. Selecting Email One-Time Password (OTP) is recommended. However, you can also use your Amazon.com retail account to login.\nFollow the prompts to complete the login process. Once you have successfully logged in. You will see the following screen. Select AWS Console\nA window will open. From the window select Open AWS Console\nThis will open the AWS Console in a new window on your web browser. If you can see the home page for the AWS Console as depicted below you have successfully logged into your AWS account.\nNow that you have successfully logged into your AWS account and are able to access the AWS Console, let\u0026rsquo;s begin the next step Environment Set Up\n"
},
{
	"uri": "/open-search-log-analytics/1_getting_started/aws_event.html",
	"title": "AWS Event",
	"tags": [],
	"description": "",
	"content": "\rOnly complete this section if you are at an AWS hosted event (such as re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee). If you are running the workshop on your own, go to Self Paced.\n\rLog into AWS Console via. AWS Workshop Portal Your instructor has already created an AWS account for you. Your instructor should provide you a participant hash.\nOnce you have your participant hash go to https://dashboard.eventengine.run/\nEnter your participant has and select Accept Terms \u0026amp; Login\nSelect how you want to log in. Selecting Email One-Time Password (OTP) is recommended. However, you can also use your Amazon.com retail account to login.\nFollow the prompts to complete the login process. Once you have successfully logged in. You will see the following screen. Select AWS Console\nA window will open. From the window select Open AWS Console\nThis will open the AWS Console in a new window on your web browser. If you can see the home page for the AWS Console as depicted below you have successfully logged into your AWS account.\nNow that you have successfully logged into your AWS account and are able to access the AWS Console, let\u0026rsquo;s begin the next step Environment Set Up\n"
},
{
	"uri": "/open-search-alerting/2_environment_set_up/cloudformation_automated.html",
	"title": "CloudFormation",
	"tags": [],
	"description": "",
	"content": "Step 1 - Deploy CloudFormation Template  Right click the Launch Stack button below and open the link in a new tab.  \n Navigate to the new tab you just opened. You should see a screen similar to the one below   Click on the Next button\n  Populate EmailParameter with a valid email address to send alerts.   Continue to click on the Next buttons until you arrive at the final review screen. Scroll to the bottom of this page\n  At the bottom of the page click on I acknowledge that AWS CloudFormation might create IAM resources with custom names\n  Click on Create stack\n  This will begin the process of deploy an Amazon OpenSearch service, SNS topic, and Cloud9 environment. You will automatically be redirected to the following screen\nThe deployment will take 5 - 10 minutes to complete. You can click on the refresh button and view the status of the deployment. The initial status will be CREATE_IN_PROGRESS when the status is CREATE_COMPLETE proceed to step 2 below.\nBe sure to check the Outputs tab of the os-alerting stack for environment info. Step 2 - Confirm SNS Subscription You will receive an email confirming a subscription to your SNS topic. Click Confirm subscription. Proceed to Create OpenSearch Index\n"
},
{
	"uri": "/open-search-cross-cluster-replication/2_environment_set_up/cloudformation_automated.html",
	"title": "CloudFormation",
	"tags": [],
	"description": "",
	"content": "Step 1 - Deploy CloudFormation Template  Right click the Launch Stack button below and open the link in a new tab.  \n Navigate to the new tab you just opened. You should see a screen similar to the one below   Continue to click on the Next buttons until you arrive at the final review screen. Scroll to the bottom of this page\n  Click on Create stack\n  This will begin the process of deploy multiple domains for Amazon OpenSearch service and a Cloud9 environment. You will automatically be redirected to the following screen\nThe deployment will take 5 - 10 minutes to complete. You can click on the refresh button and view the status of the deployment. The initial status will be CREATE_IN_PROGRESS when the status is CREATE_COMPLETE proceed to step 2 below.\nBe sure to check the Outputs tab of the os-cross-cluster-replication stack for environment info. Proceed to Setup a Cross-Cluster Connection\n"
},
{
	"uri": "/anomaly-detection-w-msk/2_environment_set_up/cloudformation_automated.html",
	"title": "CloudFormation (Automated)",
	"tags": [],
	"description": "",
	"content": "Step 1 - Deploy CloudFormation Template  Right click the Launch CloudFormation Stack button below and open the link in a new tab.  \nNavigate to the new tab you just opened. You should see a screen similar to the one below  Click on the Next button Continue to click on the Next buttons until you arrive at the final review screen. Scroll to the bottom of this page At the bottom of the page click on I acknowledge that AWS CloudFormation might create IAM resources with custom names Click on Create stack  This will begin the process of deploying the required resources. You will automatically be redirected to the following screen\nThe deployment will take 5 - 10 minutes to complete. You can click on the refresh button and view the status of the deployment. The initial status will be CREATE_IN_PROGRESS when the status is CREATE_COMPLETE. Click on the Outputs section of the CloudFormation stack and copy the values of the output. The output values will be used in subsequent parts of this workshop.\nStep 2 - Update MSK Network Security Group Cloud9 and MSK from a network point of view, must be able to communicate with each other. To achieve this you must update the network security group for MSK to accept incoming traffic from the Cloud9 security group.\n Navigate to the Security Group page in the AWS console Select the security group with the name msk security group Click on the Inbound rules tab Click on Edit inbound rules  You will be redirected to the edit inbound rules page\nClick on Add rule Select All traffic from the type drop down Select the security group that starts with aws-cloud9-msk-works as the source Click on Save rules  The msk security group will now accept inbound network traffic originating from the cloud-9 security group\nStep 3 - Create Kafka Topic We need to create a topic in Kafka that we can later send log data to.\n Navigate to the MSK page in the AWS console Select the msk-workshop-cluster  Click on View client information  Copy the plaintext Apache ZooKeeper connection information  Navigate to the Cloud9 console Click on Open IDE  In the Cloud9 environment navigate and open Kafka_OpenSearch_Anomaly_Detection/Kafka/1_create_topic.py  Run yes | sudo yum install java-1.8.0 in the Cloud9 terminal Run wget https://archive.apache.org/dist/kafka/2.6.2/kafka_2.12-2.6.2.tgz Run tar -xzf kafka_2.12-2.6.2.tgz Run cd kafka_2.12-2.6.2  Update the bin/kafka-topics.sh --create --zookeeper \u0026lt;ZookeeperConnectString\u0026gt; --replication-factor 1 --partitions 1 --topic ApplicationMetricTopic replace the \u0026lt;ZookeeperConnectString\u0026gt; with the plaintext Apache ZooKeeper connection information you copied down on Step 4  Run the updated command in the Cloud9 terminal\nThe command will return a result Created topic ApplicationMetricTopic\nStep 4 - Create OpenSearch Index Before we can send data to OpenSearch we need to create an index. You will run a python script from Cloud9 that will create an index in the OpenSearch domain.\nIf you are not already in the Cloud9 console\n Navigate to the Cloud9 in the AWS console Under the msk-workshop-cloud9 environment click on Open IDE  If you are already in the Cloud9 console\nIn the Cloud9 environment navigate and open Kafka_OpenSearch_Anomaly_Detection/OpenSearch/1_create_index.py Replace the \u0026lt;opensearch_domain_endpoint_url\u0026gt; with the value of the OSDomainURL key from the CloudFormation stack output and save the file Run the following command on the Cloud9 terminal cd ~ Run the following command on the Cloud9 terminal pip install requests Run the python script by issuing the following command in the Cloud9 terminal python environment/Kafka_OpenSearch_Anomaly_Detection/OpenSearch/1_create_index.py  Step 5 - Map IAM Role to OpenSearch Role Later in the lab when we upload data to OpenSearch the IAM role we will use needs access in OpenSearch. We can run a python script in Cloud9 map the IAM role to the OpenSearch role\nIf you are not already in the Cloud9 console\n Navigate to the Cloud9 in the AWS console Under the msk-workshop-cloud9 environment click on Open IDE  If you are already in the Cloud9 console\nIn the Cloud9 environment navigate and open Kafka_OpenSearch_Anomaly_Detection/OpenSearch/2_map_IAM_user.py Replace the \u0026lt;os_domain_url\u0026gt; with the value of the OSDomainURL key from the CloudFormation stack output Replace the \u0026lt;IAM_user/role_arn\u0026gt; with the value of the IAMUserARN key from the CloudFormation stack output and save the file Run the python script by issuing the following command in the Cloud9 terminal python environment/Kafka_OpenSearch_Anomaly_Detection/OpenSearch/2_map_IAM_user.py  Our AWS environment set up. When you are ready begin the next step Configure Lambda\n"
},
{
	"uri": "/open-search-fluentd/2_environment_set_up/cloudformation_automated.html",
	"title": "CloudFormation (Automated)",
	"tags": [],
	"description": "",
	"content": "Step 1 - Deploy CloudFormation Template  Right click the Launch CloudFormation Stack button below and open the link in a new tab.  \nNavigate to the new tab you just opened. You should see a screen similar to the one below  Click on the Next button Continue to click on the Next buttons until you arrive at the final review screen. Scroll to the bottom of this page At the bottom of the page click on I acknowledge that AWS CloudFormation might create IAM resources with custom names Click on Create stack  This will begin the process of deploy an Amazon OpenSearch service, Kinesis Data Firehose and Cloud9 environment. You will automatically be redirected to the following screen\nThe deployment will take 5 - 10 minutes to complete. You can click on the refresh button and view the status of the deployment. The initial status will be CREATE_IN_PROGRESS when the status is CREATE_COMPLETE. Click on the Outputs section of the CloudFormation stack and copy the values of the output. The output values will be used in subsequent parts of this workshop.\nStep 2 - Map IAM Role with OpenSeach Role You now have an OpenSearch domain and Cloud9 environment created. In order to send logs to OpenSearch via Fluentd you need to grant the IAM role that Fluentd will use permissions in OpenSearch.\nThe CloudFormation stack already created an IAM role for us. you need to map the IAM role to an OpenSearch role. To map the role you can run a python script in Cloud9 that will use the OpenSearch APIs to map the IAM role the all_access role in OpenSearch.\nFor the purposes of this lab you are mapping our IAM role to the all_access role in OpenSearch. In a production environment more limited permissions should be granted to the IAM role Fluentd will use\n\rTo map the IAM role to the OpenSearch role follow the steps below\n Go to the Cloud9 Console Click on Open IDE  Once you have opened the Cloud9 IDE navigate the folder structure on the left hand menu to and open the Flentd_Examples folder. Then the Cloud9_Apache_Logs_OpenSearch sub-folder. In this folder open the map_IAM_user.py file.\nIn the map_IAM_user.py file\nReplace the value of the os_url variable with the value of the OSDomainURL key from the CloudFormation stack outputs Replace the value of the iam_arn variable with the value of the IAMUserARN key from the CloudFormation stack outputs Save the python file  You can now run the map_IAM_user.py script by executing the following commands in the Cloud9 terminal\npip install requests python Fluentd_Examples/Cloud9_Apache_Logs_OpenSearch/map_IAM_user.py  Our AWS environment set up. When you are ready begin the next step Configure Fluentd\n"
},
{
	"uri": "/open-search-log-analytics/2_environment_set_up/cloudformation_automated.html",
	"title": "CloudFormation (Automated)",
	"tags": [],
	"description": "",
	"content": "Step 1 - Deploy CloudFormation Template  Right click the Launch CloudFormation Stack button below and open the link in a new tab.  \nNavigate to the new tab you just opened. You should see a screen similar to the one below  Click on the Next button Continue to click on the Next buttons until you arrive at the final review screen. Scroll to the bottom of this page At the bottom of the page click on I acknowledge that AWS CloudFormation might create IAM resources with custom names Click on Create stack  This will begin the process of deploy an Amazon OpenSearch service, Kinesis Data Firehose and Cloud9 environment. You will automatically be redirected to the following screen\nThe deployment will take 5 - 10 minutes to complete. You can click on the refresh button and view the status of the deployment. The initial status will be CREATE_IN_PROGRESS when the status is CREATE_COMPLETE proceed to step 2 below.\nStep 2 - Configure Identity Access Management (IAM) Permissions We now have an OpenSearch domain and Kinesis Firehose created. In order to send logs to OpenSearch via Kinesis Data Firehose we need to grant the IAM role that firehose uses permissions in OpenSearch.\nMap IAM Role with OpenSeach Role  Go to the OpenSearch Console Click on the workshop-domain OpenSearch domain you created earlier Click on the OpenSearch Dashboard URL. This should open the URL in a web browser window  You will be prompted to log in. Using the user name OSMasterUser and password AwS#OpenSearch1 log in If an additional pop up window is present after login asking about data upload click on Explore on my own If an additional pop up windows is present asking you to select your tenant select Global and click on Confirm  You should now see a window that looks like this\nClick on and expand the hamburger menu on the side bar of the OpenSearch home page Under the OpenSearch Plugins section click on Security  On the security page click on Roles from the left hand menu  On the roles page search for and click on all_access  On the all_access role page click on Mapped users Under the mapped users page click on Manage mapping  On the manage mapping page we need to map the IAM role the is used by Kinesis Data Firehose to the all_access OpenSearch role. This will give Kinesis Firehose the permissions it need to create, update indexes and write data.\nFor the purposes of this lab we will give Kinesis Firehose all_access in OpenSearch.\nWe need to find the ARN of the IAM role Kinesis Firehose is using. Keeping the Manage mapping page open in your browser, navigate to a new tab and\nGo to the Kinesis Firehose Console Click on the workshop-firehose listed. This is the Kinesis Data Fire hose we created earlier Click on the Configuration section  On the configuration page scroll down to the permissions section Click on the IAM role  This will open a new window in your web browser. Copy down the ARN of the IAM role  Navigate back to the OpenSearch map user tab Enter the ARN we copied in step 18 and paste it in the backend roles section of OpenSearch console page Click on Map  Our AWS environment set up. When you are ready begin the next step Send Log Data to Kinesis Fire Hose\n"
},
{
	"uri": "/open-search-log-analytics/2_environment_set_up/console_manual.html",
	"title": "Console Deploy (Manual)",
	"tags": [],
	"description": "",
	"content": "\rOnly complete this section if you intend to set up your AWS environment manually via. the AWS console. If you have completed CloudFormation (Automated) skip this section and proceed to Send Log Data to Kinesis Fire Hose\n\rStep 1 - Create an OpenSearch Domain  Go to the OpenSearch Console Click on Create domain  Enter the name workshop-domain for the OpenSearch Domain Under the deployment type section, select Development and testing Under the network section, select Public access Under the fine-grained access control section select Create master user For the username enter OSMasterUser For the password enter AwS#OpenSearch1 Under the access policy section, select Only use fine-grain access control Leave all other settings at the default selections Click on Create  It will take approximately 5 - 10 minutes for your OpenSearch domain to be created. Upon successful creation you will see your OpenSearch domain status is active\nDo not proceed to the next step until you confirm that your domain status is active\nStep 2 - Create a Kinesis Firehose  Go to the Kinesis Firehose Console Click on Create delivery stream  Under the choose source and destination section for the source, select Direct PUT for the destination select Amazon OpenSearch Service Under the delivery stream name section name the stream workshop-firehose  Under the destination settings for the OpenSearch service domain, click on Browse and select the OpenSearch domain workshop-domain this is the OpenSearch domain we created in the previous step Name the index workshop-log Select Every hour for the Index rotation. This will produce a next index every hour  Expand the Buffer hints section Adjust the buffer interval to 60 seconds. This will write data from Kinesis Firehose to OpenSearch every 60 seconds  Under the backup settings under the S3 backup bucket click on Create. This will (in a new browser window) open the create S3 bucket web page  On the create bucket page provide a bucket name. You may name the bucket any valid name Click on Create bucket at the bottom of the page. Leaving all of the S3 settings the the default selections Return the browser window that we were using to configure our Kinesis Firehose and under the backup setting section for the S3 backup bucket, click on Browse Select the bucket you just created. If you do not see the bucket listed click on the small refresh button in the top right corner of the window that pops us when you click on the browse button  At the bottom of the page click on Create delivery stream leave all other settings at the default selections  Step 3 - Configure Identity Access Management (IAM) Permissions We now have an OpenSearch domain and Kinesis Firehose created. In order to send logs to OpenSearch via Kinesis Data Firehose we need to grant the IAM role that firehose uses permissions in OpenSearch.\nMap IAM Role with OpenSeach Role  Go to the OpenSearch Console Click on the workshop-domain OpenSearch domain you created earlier Click on the OpenSearch Dashboard URL. This should open the URL in a web browser window  You will be prompted to log in. Using the user name OSMasterUser and password AwS#OpenSearch1 log in If an additional pop up window is present after login asking about data upload click on Explore on my own If an additional pop up windows is present asking you to select your tenant select Global and click on Confirm  You should now see a window that looks like this\nClick on and expand the hamburger menu on the side bar of the OpenSearch home page Under the OpenSearch Plugins section click on Security  On the security page click on Roles from the left hand menu  On the roles page search for and click on all_access  On the all_access role page click on Mapped users Under the mapped users page click on Manage mapping  On the manage mapping page we need to map the IAM role the is used by Kinesis Data Firehose to the all_access OpenSearch role. This will give Kinesis Firehose the permissions it need to create, update indexes and write data.\nFor the purposes of this lab we will give Kinesis Firehose all_access in OpenSearch.\nWe need to find the ARN of the IAM role Kinesis Firehose is using. Keeping the Manage mapping page open in your browser, navigate to a new tab and\nGo to the Kinesis Firehose Console Click on the workshop-firehose listed. This is the Kinesis Data Fire hose we created earlier Click on the Configuration section  On the configuration page scroll down to the permissions section Click on the IAM role  This will open a new window in your web browser. Copy down the ARN of the IAM role  Navigate back to the OpenSearch map user tab Enter the ARN we copied in step 18 and paste it in the backend roles section of OpenSearch console page Click on Map  Our AWS environment set up. When you are ready begin the next step Send Log Data to Kinesis Fire Hose\n"
},
{
	"uri": "/open-search-alerting/2_environment_set_up/create_index.html",
	"title": "Create OpenSearch Index",
	"tags": [],
	"description": "",
	"content": "You will need to create a sample index in OpenSearch so you can store and query data for an alert. To do this, you will run a Python application in our Cloud9 environment to create the index.\nStep 1 - Create Index  Navigate to the Cloud9 Console Click on Open IDE under the already created workshop-cloud9 environment.  The OpenSearch_API_Examples repository will be automatically pulled down to your environment. Do the following within the Cloud9 console.\n Open the file OpenSearch_API_Examples/Alerting/1_create_index.py Update the os_url variable to your OpenSearch instance. This is the value of the OSDomainURL key from the CloudFormation stack output. Go to File -\u0026gt; Save to save the changes to 1_create_index.py  In the terminal window, run the following commands:\npip install requests python OpenSearch_API_Examples/Alerting/1_create_index.py  Step 2 - Check that the Index was Created via. OpenSearch Dashboard The image below highlights the commands.\nThis command creates and index for use in this workshop\n Open the OpenSearch Dashboard using the URL provided in the value for the key DashboardURL in the CloudFormation outputs  Once you open the dashboard URL you will be prompted to login\nEnter OSMasterUser as the username Enter AwS#OpenSearch1 as the password Click on Log In If an additional pop up window is present after login asking about data upload click on Explore on my own If an additional pop up windows is present asking you to select your tenant select Global and click on Confirm  Once you have logged in to the OpenSearch dashboard\nClick on the top left hand menu on the OpenSearch dashboard Click on Index Management under the OpenSearch Plugins section  Click on Indices  You should be able to see an index alert-1 is present\nWhen you are ready move on to the next step Create Alert\n"
},
{
	"uri": "/open-search-cross-cluster-replication/",
	"title": "Cross Cluster Replication",
	"tags": [],
	"description": "",
	"content": "Cross Cluster Replication Workshop This workshop demonstrates how to replicate indexes, mapping, and metadata from one OpenSearch Service domain to another. This follows an active-passive replication model where a following index pulls data from a leader index to keep the indexes in sync.\nIn this workshop you will implement the following architecture:\nThe architecture uses Python applications run from a Cloud9 development environment to create a sample index and to send sample data to OpenSearch. You will set up replication between the leader and follower domains and observe how the index data replicates.\nWhen you are ready to being the lab navigate to Getting started\n"
},
{
	"uri": "/open-search-alerting/",
	"title": "OpenSearch Alerting",
	"tags": [],
	"description": "",
	"content": "OpenSearch Alerting Workshop This workshop demonstrates how alerts work with OpenSearch. When data from your OpenSearch indices match certain conditions, an alert can notify destinations such as SNS or Slack. For example, you may want an alert if your application reports connection timeouts to dependent server.\nIn this workshop you will implement the following architecture:\nThe architecture uses Python applications run from a Cloud9 development environment to create a sample index and to send sample data to OpenSearch. You will set up an alert in OpenSearch and send notifications using Amazon Simple Notification Service (SNS) to an email address.\nWhen you are ready to being the lab navigate to Getting started\n"
},
{
	"uri": "/anomaly-detection-w-msk/1_getting_started/self_paced.html",
	"title": "Self Paced",
	"tags": [],
	"description": "",
	"content": "\rOnly complete this section if you are running the workshop on your own. If you are at an AWS hosted event (such as re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee), continue with AWS Event.\n\rRunning the Workshop on Your Own You will need an AWS Account with web console access to complete the workshop.\nIf you do not have an AWS account Create an AWS Account\nAs pictured below you should be able to access the AWS Console\nOnce you have can access the AWS Console, lets begin the next step Environment Set Up\n"
},
{
	"uri": "/collect-log-cloud-watch/1_getting_started/self_paced.html",
	"title": "Self Paced",
	"tags": [],
	"description": "",
	"content": "\rOnly complete this section if you are running the workshop on your own. If you are at an AWS hosted event (such as re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee), continue with AWS Event.\n\rRunning the Workshop on Your Own You will need an AWS Account with web console access to complete the workshop.\nIf you do not have an AWS account Create an AWS Account\nAs pictured below you should be able to access the AWS Console\nOnce you have can access the AWS Console, lets begin the next step Enviorment Set Up\n"
},
{
	"uri": "/open-search-alerting/1_getting_started/self_paced.html",
	"title": "Self Paced",
	"tags": [],
	"description": "",
	"content": "\rOnly complete this section if you are running the workshop on your own. If you are at an AWS hosted event (such as re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee), continue with AWS Event.\n\rRunning the Workshop on Your Own You will need an AWS Account with web console access to complete the workshop.\nIf you do not have an AWS account Create an AWS Account\nAs pictured below you should be able to access the AWS Console\nOnce you have can access the AWS Console, lets begin the next step Environment Set Up\n"
},
{
	"uri": "/open-search-cross-cluster-replication/1_getting_started/self_paced.html",
	"title": "Self Paced",
	"tags": [],
	"description": "",
	"content": "\rOnly complete this section if you are running the workshop on your own. If you are at an AWS hosted event (such as re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee), continue with AWS Event.\n\rRunning the Workshop on Your Own You will need an AWS Account with web console access to complete the workshop.\nIf you do not have an AWS account Create an AWS Account\nAs pictured below you should be able to access the AWS Console\nOnce you have can access the AWS Console, lets begin the next step Environment Set Up\n"
},
{
	"uri": "/open-search-fluentd/1_getting_started/self_paced.html",
	"title": "Self Paced",
	"tags": [],
	"description": "",
	"content": "\rOnly complete this section if you are running the workshop on your own. If you are at an AWS hosted event (such as re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee), continue with AWS Event.\n\rRunning the Workshop on Your Own You will need an AWS Account with web console access to complete the workshop.\nIf you do not have an AWS account Create an AWS Account\nAs pictured below you should be able to access the AWS Console\nOnce you have can access the AWS Console, lets begin the next step Environment Set Up\n"
},
{
	"uri": "/open-search-log-analytics/1_getting_started/self_paced.html",
	"title": "Self Paced",
	"tags": [],
	"description": "",
	"content": "\rOnly complete this section if you are running the workshop on your own. If you are at an AWS hosted event (such as re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee), continue with AWS Event.\n\rRunning the Workshop on Your Own You will need an AWS Account with web console access to complete the workshop.\nIf you do not have an AWS account Create an AWS Account\nAs pictured below you should be able to access the AWS Console\nOnce you have can access the AWS Console, lets begin the next step Environment Set Up\n"
},
{
	"uri": "/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]